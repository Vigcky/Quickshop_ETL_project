# ğŸª Quickshop ETL Project  

End-to-end **ETL (Extractâ€“Transformâ€“Load)** data pipeline for **QuickShop**, featuring:  
âœ… Data ingestion from CSV  
âœ… Schema validation & transformation  
âœ… PostgreSQL persistence  
âœ… Flask REST API  
âœ… Apache Airflow orchestration  
âœ… SQL analytics queries  
âœ… Full Dockerized environment  

---

## ğŸš€ Overview  

This project simulates a real-world **data engineering workflow** for an online retail platform.  
The pipeline reads raw CSV data (orders, products, inventory), applies schema validation & transformation, and writes processed data to a PostgreSQL database or Parquet files.  

Airflow then orchestrates this ETL daily, while Flask provides API endpoints for managing and querying data.  

---

## âš™ï¸ Features  

| Component | Description |
|------------|--------------|
| ğŸ§© **Python ETL** | Reads CSVs, validates schema, transforms orders (e.g. `order_total = qty * unit_price`) |
| ğŸ§± **Schema Validation** | Implemented using `pydantic` models |
| ğŸ’¾ **PostgreSQL** | Stores transformed data |
| ğŸ”„ **Airflow DAG** | Automates daily ETL & summary generation |
| ğŸŒ **Flask REST API** | CRUD endpoints for orders (`/orders`) |
| ğŸ§® **SQL Scripts** | Analytical queries for revenue, performance, cohorts |
| ğŸ§ª **Pytest** | Unit-tested extract, transform, load components |
| ğŸ³ **Docker Compose** | Orchestrates Flask, Airflow, and PostgreSQL containers |

---

## ğŸ—ï¸ Architecture  

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚        CSV Input          â”‚
               â”‚  (orders, products, inv)  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    [Extract: Pandas]
                            â”‚
                    [Transform: Pydantic]
                            â”‚
                    [Load: PostgreSQL/Parquet]
                            â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                         â”‚
            [Airflow DAGs] [Flask API Layer]
       â”‚                                         â”‚
        [Automates Daily ETL] [CRUD + Reporting]
       â”‚                                         â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                [SQL Analytical Reports]


---

## ğŸ§© Project Structure  

Quickshop_ETL_Project/
â”œâ”€â”€ Quickshop_ETL/
â”‚ â”œâ”€â”€ extract.py
â”‚ â”œâ”€â”€ transform.py
â”‚ â”œâ”€â”€ load.py
â”‚ â”œâ”€â”€ schema.py
â”‚ â”œâ”€â”€ analytics.py
â”‚ â””â”€â”€ init.py
â”‚
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ quickshop_daily_pipeline.py
â”‚
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ daily_revenue.sql
â”‚ â”œâ”€â”€ product_performance.sql
â”‚ â”œâ”€â”€ inventory_alerts.sql
â”‚ â””â”€â”€ cohort_retention.sql
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ products.csv
â”‚ â”œâ”€â”€ inventory.csv
â”‚ â””â”€â”€ orders_20251101.csv
â”‚
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_extract.py
â”‚ â”œâ”€â”€ test_transform.py
â”‚ â”œâ”€â”€ test_load.py
â”‚ â””â”€â”€ conftest.py
â”‚
â”œâ”€â”€ run_etl.py
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ init.sql
â””â”€â”€ README.md

---

## âš¡ Setup Options  

### ğŸ§± Option 1 â€” Manual Setup (Recommended for testing)  

1ï¸âƒ£ **Install dependencies**  
```bash
uv sync
or with pip:
    pip install -r requirements.txt
2ï¸âƒ£ **Create PostgreSQL database manually**
psql -U postgres
CREATE DATABASE quickshop;
\c quickshop
\i init.sql
3ï¸âƒ£ Run ETL manually:
python run_etl.py --input-dir data --output-dir out \
  --start-date 2025-11-01 --end-date 2025-11-05 \
  --format db \
  --db-url postgresql+psycopg2://postgres:182003@localhost:5432/quickshop

ğŸ³ Option 2 â€” Docker Compose (All-in-one)
     docker-compose up --build

Then access:

Flask API â†’ http://localhost:5000/orders

Airflow UI â†’ http://localhost:8080
 (login: admin / admin)

PostgreSQL â†’ localhost:5432 (user: postgres, pass: 182003)


ğŸ’¾ Flask API Endpoints
| Method | Endpoint  | Description     |
| ------ | --------- | --------------- |
| GET    | `/orders` | List all orders |
| POST   | `/orders` | Add new order   |
| GET    | `/health` | Health check    |



â±ï¸ Airflow Orchestration

DAG Name: quickshop_daily_pipeline

Schedule: Daily (@daily)

Tasks:

Extract data from /data

Transform and validate

Load into PostgreSQL

Generate summary JSON report

File: dags/quickshop_daily_pipeline.py


ğŸ“Š SQL Analytical Queries

| File                      | Purpose                                 |
| ------------------------- | --------------------------------------- |
| `daily_revenue.sql`       | Computes daily revenue & top categories |
| `product_performance.sql` | Tracks units sold & return rates        |
| `inventory_alerts.sql`    | Finds low-stock products                |
| `cohort_retention.sql`    | Cohort retention trends                 |


ğŸ§ª Testing

Run all unit tests:

pytest -v
